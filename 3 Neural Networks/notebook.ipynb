{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1. Introduction to Neural Networks\n",
    "\n",
    "**Hello everyone!** \n",
    "\n",
    "Imagine you have lots of little helpers (called \"neurons\") who pass simple messages (numbers) to each other. Together, they can learn to solve problems—just like a team figuring out puzzles. This is what a **Neural Network** does!\n",
    "\n",
    "In this notebook, we will explore:\n",
    "1. What neurons are.\n",
    "2. How they connect into layers.\n",
    "3. How they learn through something called **gradient descent** and **backpropagation**.\n",
    "4. How to build a simple neural network in Python to recognise flower types (using the Iris dataset as an example).\n",
    "\n",
    "We will keep things as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons: The Little Helpers\n",
    "\n",
    "**Think of a neuron like a small box** that waits for numbers (information) to come in. \n",
    "It does a tiny calculation (like adding or multiplying) and then passes the result along to another neuron.\n",
    "\n",
    "- Each neuron has weights (like small switches) and a bias (a little push).\n",
    "- When numbers come in, they get multiplied by weights, added together, and then the bias is added. \n",
    "- The result might get squashed with a function (called the activation function), then sent to the next neuron.\n",
    "\n",
    "This is how one neuron works. A neural network has many such neurons, arranged in **layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers: Stacks of Neurons\n",
    "\n",
    "**A layer is a group of neurons** side-by-side, each doing its tiny calculation. \n",
    "- The first layer receives the original input (like a picture or some numbers about flowers).\n",
    "- The middle layers are called **hidden layers** (because we usually don't see what they do directly).\n",
    "- The final layer makes a **prediction**—like, \"This flower is setosa,\" or \"This flower is versicolor,\" etc.\n",
    "\n",
    "Sometimes, many layers in a network are called **deep** learning (because it's deep with lots of layers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Backpropagation: How Neurons Learn\n",
    "\n",
    "**Gradient Descent**:\n",
    "- Imagine you are on a hill in the fog and want to get to the bottom (the best solution).\n",
    "- You can't see far, so you take small steps downhill (the direction where the ground slopes down the most).\n",
    "- Eventually, step by step, you get to the lowest point you can find.\n",
    "\n",
    "**Backpropagation**:\n",
    "- It's like checking how each little step affects the total error (like how far you are from the goal at the bottom).\n",
    "- If a neuron didn't do its job well, we adjust its weights and bias.\n",
    "- We do this backwards, layer by layer, so each neuron learns how to do better next time.\n",
    "\n",
    "This is repeated many times until the network is good at guessing or predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a small flower dataset (the famous Iris dataset).\n",
    "# It has measurements of petals and sepals, and the type of flower (3 species).\n",
    "\n",
    "# numpy helps with math on big lists of numbers.\n",
    "import numpy as np #\n",
    "\n",
    "# pandas is for reading and handling data tables.\n",
    "import pandas as pd #\n",
    "\n",
    "# matplotlib and seaborn help us draw pictures of our data.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow/Keras will build and train our neural network.\n",
    "# For our neural network, we'll use TensorFlow / Keras (a simple library for neural networks).\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about this piece of code, it is to make sure that the dataset is installed correctly\n",
    "# If you get an error in the following cell, run this cell, restart the kernel and then run the following cell again\n",
    "import certifi\n",
    "import os\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris data directly from seaborn for simplicity\n",
    "iris_data = sns.load_dataset(\"iris\")\n",
    "\n",
    "# The Iris dataset has 150 flowers with columns: sepal_length, sepal_width, petal_length, petal_width, and species.\n",
    "# The species can be one of three types: setosa, versicolor, or virginica.\n",
    "\n",
    "# Let's see the first few rows\n",
    "print(iris_data.head())\n",
    "\n",
    "# Check how many rows and columns\n",
    "print(\"\\nDataset shape:\", iris_data.shape)\n",
    "\n",
    "# Check what species we have\n",
    "print(\"\\nSpecies distribution:\")\n",
    "print(iris_data[\"species\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to separate the \"input\" (measurements) from the \"output\" (species).\n",
    "X = iris_data.drop(\"species\", axis=1)  # All columns except 'species'\n",
    "y = iris_data[\"species\"]  # Just the 'species' column\n",
    "\n",
    "# For the output (y), we need to turn the species names into numbers.\n",
    "# Keras can handle this with \"one-hot encoding\", but let's do it manually with pandas \"get_dummies\".\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Now, X contains the measurements (4 columns).\n",
    "# y_encoded contains something like:\n",
    "#   setosa  versicolor  virginica\n",
    "# 0    1        0         0\n",
    "# 1    1        0         0\n",
    "# ... and so on\n",
    "print(\"Encoded species (first few rows):\")\n",
    "print(y_encoded.head())\n",
    "\n",
    "\"\"\"\n",
    "Neural networks want numbers. So we convert the species into columns of 0/1 (one-hot encoding).\n",
    "\n",
    "For example, if a flower is setosa, it might be represented as [1, 0, 0].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data into training set (to learn) and test set (to check if it learned well).\n",
    "# We'll use 80% for training, 20% for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make a small network:\n",
    "# Input layer -> Hidden layer -> Output layer.\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(\n",
    "            8, activation=\"relu\", input_shape=(4,)\n",
    "        ),  # 4 inputs (4 measurements), 8 neurons\n",
    "        layers.Dense(8, activation=\"relu\"),  # Another hidden layer with 8 neurons\n",
    "        layers.Dense(\n",
    "            3, activation=\"softmax\"\n",
    "        ),  # 3 outputs (setosa, versicolor, virginica)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model:\n",
    "# - loss='categorical_crossentropy' because we have more than two categories\n",
    "# - optimizer='adam' is a popular version of gradient descent\n",
    "# - metrics=['accuracy'] to track how many we get right\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Neural network created and compiled!\")\n",
    "model.summary()  # Let's see what our model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train for some epochs (think of 1 epoch as seeing all training data once).\n",
    "# We'll keep it small, e.g., 50 epochs, so it runs quickly.\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,  # our inputs for training\n",
    "    y_train,  # our one-hot encoded output labels\n",
    "    epochs=50,  # how many times to see all data\n",
    "    batch_size=8,  # how many samples to process before updating weights\n",
    "    validation_split=0.2,  # split part of the training set for validation\n",
    "    verbose=0,  # set to 1 if you want to see the progress bar\n",
    ")\n",
    "\n",
    "# Let's look at final accuracy on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the training and validation accuracy over epochs.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap and Next Steps\n",
    "\n",
    "**Great job!** We've built a small neural network to classify flowers in the Iris dataset. \n",
    "We learned:\n",
    "- How neurons are like little helpers that perform small calculations.\n",
    "- How layers stack these neurons.\n",
    "- How networks learn by taking small steps down the \"hill\" (gradient descent).\n",
    "- How \"backpropagation\" helps each neuron fix its mistakes.\n",
    "\n",
    "**What next?**\n",
    "- Try adding more layers to see if accuracy changes.\n",
    "- Try changing the activation function to see what happens.\n",
    "- Use a different dataset (e.g., digits or something else)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple (Child-Like) Summary\n",
    "\n",
    "1. **Neuron**: A tiny friend who reads numbers, does a small sum, and sends out a number.\n",
    "2. **Layer**: A bunch of friends (neurons) side by side.\n",
    "3. **Network**: Lots of layers stacked to solve a problem together.\n",
    "4. **Gradient Descent**: Tiny steps to find the best path (like going downhill).\n",
    "5. **Backpropagation**: Checking who made mistakes and fixing them from the end back to the start.\n",
    "\n",
    "This is how a neural network learns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Generative AI for Neural Network Experiments\n",
    "\n",
    "If you get stuck or want to **try different approaches**, you can use prompt engineering with a tool like ChatGPT. \n",
    "\n",
    "Example prompt:\n",
    "\"Please show me how to build a deeper neural network in Keras for the Iris dataset, with 3 hidden layers. Use 'relu' activation and show me how to evaluate with accuracy and confusion matrix. Provide code in a Jupyter Notebook format.\"\n",
    "\n",
    "By giving **specific instructions**, you often get detailed code you can copy, run, and learn from. Happy experimenting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MNIST Digit Classification with a Neural Network\n",
    "\n",
    "This part of the notebook demonstrates how to:\n",
    "1. Load the MNIST dataset (images of handwritten digits).\n",
    "2. Prepare (reshape and normalise) the data.\n",
    "3. Build a basic Neural Network using TensorFlow/Keras.\n",
    "4. Train the model and evaluate its performance.\n",
    "5. Visualise predictions and errors.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST dataset is provided within Keras, so it's easy to load.\n",
    "# It has 60,000 training images and 10,000 testing images, each of a handwritten digit from 0 to 9.\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Training set shape (images):\", X_train.shape)\n",
    "print(\"Training set shape (labels):\", y_train.shape)\n",
    "print(\"Test set shape (images):\", X_test.shape)\n",
    "print(\"Test set shape (labels):\", y_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "Each image is 28×28 pixels in grey scale.\n",
    "Labels range from 0 to 9, indicating which digit is pictured.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few sample images to get an idea of what digits look like\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(1, 6, i + 1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "We display a few images from the training set.\n",
    "The true label is shown in each subplot’s title.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reshape the images\n",
    "#    Currently, X_train has shape (60000, 28, 28).\n",
    "#    Many neural networks expect a flat array (784 = 28*28) for each image\n",
    "#    if we are building a simple Dense-based network.\n",
    "\n",
    "#    Alternatively, for a CNN, we'd keep the shape as (28,28,1).\n",
    "#    For a simple fully-connected network, we flatten it.\n",
    "X_train_flat = X_train.reshape(60000, 28 * 28)\n",
    "X_test_flat = X_test.reshape(10000, 28 * 28)\n",
    "\n",
    "# 2. Convert the pixel values from 0-255 to float32 and normalise to 0-1\n",
    "X_train_flat = X_train_flat.astype(\"float32\") / 255.0\n",
    "X_test_flat = X_test_flat.astype(\"float32\") / 255.0\n",
    "\n",
    "# 3. Convert labels (0-9) to one-hot encoding\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"Data reshaped and normalised.\")\n",
    "print(\"Label data one-hot encoded.\")\n",
    "\n",
    "\"\"\"\n",
    "Reshape: We flatten each image into a 1D vector of length 784.\n",
    "Normalisation: We scale pixel intensities from [0, 255] to [0, 1] for faster and more stable training.\n",
    "One-hot encoding: Each label (e.g., 7) becomes [0,0,0,0,0,0,0,1,0,0].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a simple feed-forward network:\n",
    "#  - Input layer: 784 inputs (flattened pixels)\n",
    "#  - Hidden layer: 128 neurons, ReLU activation\n",
    "#  - Output layer: 10 neurons, softmax activation\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(128, activation=\"relu\", input_shape=(784,)),  # hidden layer\n",
    "        Dense(10, activation=\"softmax\"),  # output layer\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()  # Show the model structure\n",
    "\n",
    "\"\"\"\n",
    "Dense(128): A layer of 128 fully connected neurons, each receiving input from all 784 pixels.\n",
    "\n",
    "ReLU: A common activation function (Rectified Linear Unit).\n",
    "\n",
    "Dense(10, softmax): Outputs a probability distribution across 10 classes (digits 0–9).\n",
    "Compile\n",
    "    loss: 'categorical_crossentropy' for multi-class classification.\n",
    "    optimizer: 'adam', a widely used optimiser that employs gradient descent methods.\n",
    "    metrics: 'accuracy' to gauge correct predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network on the training data, validating on a portion of it (validation_split=0.1).\n",
    "history = model.fit(\n",
    "    X_train_flat,\n",
    "    y_train_cat,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "\"\"\"\n",
    "epochs=10: The entire training set is shown to the network 10 times.\n",
    "batch_size=128: The network updates weights after every 128 samples.\n",
    "validation_split=0.1: 10% of the training data is held out (not used for training) so we can monitor the model’s performance as it trains.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs validation accuracy over epochs to see if our model is learning well.\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "We track accuracy (which should go up) and loss (which should go down) as we train.\n",
    "If validation metrics diverge, it might indicate overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_flat, y_test_cat, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "The test data was not used during training.\n",
    "Test accuracy is a good indicator of how well the model recognises digits it has never seen.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see which digits are often confused with others using a confusion matrix.\n",
    "\n",
    "# 1. Predict classes on the test set\n",
    "y_pred_probs = model.predict(X_test_flat)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# 2. Build the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# 3. Visualise the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Digit\")\n",
    "plt.ylabel(\"True Digit\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Print classification report (precision, recall, etc.)\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "\"\"\"\n",
    "- predict: The network returns probabilities for each class.\n",
    "- argmax picks the class (0–9) with the highest probability for each image.\n",
    "- confusion_matrix: Diagonal elements show correct predictions; off-diagonal elements show misclassifications.\n",
    "- classification_report: Summarises precision, recall, and F1-score for each digit class.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's randomly pick a few test images and see how the model performs.\n",
    "# We will display the model's predicted digit vs. the true digit.\n",
    "# A small visual sanity check to confirm if the predicted label matches the actual digit.\n",
    "\n",
    "indices = np.random.choice(len(X_test_flat), 6, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = X_test[idx]\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred_classes[idx]\n",
    "\n",
    "    plt.subplot(1, 6, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Next Steps\n",
    "\n",
    "**Congratulations!** You have successfully:\n",
    "1. Loaded the MNIST dataset.\n",
    "2. Built a simple neural network with a single hidden layer.\n",
    "3. Trained the network to recognise digits.\n",
    "4. Evaluated your model using a confusion matrix and classification metrics.\n",
    "\n",
    "### Next Steps\n",
    "- Increase the number of hidden layers or neurons to see if it improves accuracy.\n",
    "- Use Convolutional Neural Networks (CNNs), which are particularly good at image recognition.\n",
    "- Experiment with different optimisers (e.g., SGD, RMSProp) or different activation functions (e.g., sigmoid).\n",
    "\n",
    "Enjoy exploring deeper neural network techniques and other exciting datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an image and predict the digit (scale down to correct size)\n",
    "from PIL import Image\n",
    "\n",
    "# Load an image file (you can also use your own)\n",
    "img = Image.open(\"test_2.png\")\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Print the image size\n",
    "print(\"Image size (original):\", img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale\n",
    "img = img.convert(\"L\")\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize (the neural network has specific input size requirements)\n",
    "# The MNIST dataset images are 28x28 pixels.\n",
    "# We'll use the \"ANTIALIAS\" filter to preserve image quality.\n",
    "img = img.resize((28, 28))\n",
    "\n",
    "# Draw the image\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Invert the pixel values (MNIST dataset has white digits on black background)\n",
    "img_array = 255 - img_array\n",
    "\n",
    "# Normalize the pixel values (0-255 -> 0-1)\n",
    "img_array = img_array.astype(\"float32\") / 255.0\n",
    "\n",
    "# Draw the image\n",
    "plt.imshow(img_array, cmap=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the image to a 1D array\n",
    "img_array = img_array.reshape(1, 28 * 28)\n",
    "\n",
    "# Now, we can use the model to predict the digit\n",
    "pred_probs = model.predict(img_array)\n",
    "\n",
    "print(\"Predicted probabilities:\", pred_probs)\n",
    "\n",
    "# Get the predicted class\n",
    "pred_class = np.argmax(pred_probs)\n",
    "\n",
    "print(\"Predicted class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function to predict the digit from an image file\n",
    "def predict_digit(image_path):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    img = img.convert(\"L\")\n",
    "\n",
    "    # Resize\n",
    "    img = img.resize((28, 28))\n",
    "\n",
    "    # Convert to numpy array\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    # Invert the pixel values\n",
    "    img_array = 255 - img_array\n",
    "\n",
    "    # Normalize\n",
    "    img_array = img_array.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Flatten\n",
    "    try:\n",
    "        img_array = img_array.reshape(1, 28 * 28)\n",
    "        pred_probs = model.predict(img_array)\n",
    "    # This is because we will use the same function later, and the Convolutional Neural Network expects a different shape\n",
    "    except ValueError:\n",
    "        img_array = img_array.reshape(1, 28, 28, 1)\n",
    "        pred_probs = model.predict(img_array)\n",
    "\n",
    "    # Predict\n",
    "    pred_class = np.argmax(pred_probs)\n",
    "\n",
    "    # Visualise the probabilities\n",
    "    plt.imshow(img_array.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Predicted: {pred_class}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print each probabilty in line with the predicted class\n",
    "    for i, prob in enumerate(pred_probs[0]):\n",
    "        print(f\"Probability of {i}: {prob:.4f}\")\n",
    "\n",
    "# Test the function\n",
    "image_path = \"test_3.png\"\n",
    "predicted_digit = predict_digit(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MNIST Digit Classification with a Simple CNN\n",
    "\n",
    "This section of the  notebook demonstrates:\n",
    "1. Loading the MNIST dataset.\n",
    "2. Preparing data for a Convolutional Neural Network (CNN).\n",
    "3. Defining and training a CNN with TensorFlow/Keras.\n",
    "4. Evaluating the model's performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# mnist: Quick access to the MNIST dataset through Keras.\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Sequential: A way to build models layer by layer.\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Conv2D, MaxPooling2D, Flatten, Dense, Dropout: Layers we'll use to build our CNN.\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# to_categorical: Converts labels to a one-hot encoding format (e.g., digit 7 becomes [0,0,0,0,0,0,0,1,0,0]).\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# confusion_matrix and classification_report: Evaluate how well the model performs.\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"Training set size:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set size:\", X_test.shape, y_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "X_train, y_train = images and labels for training (60,000 samples).\n",
    "X_test, y_test = images and labels for testing (10,000 samples).\n",
    "Each image is 28 x 28, and each label is an integer from 0 to 9.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few examples\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(1, 6, i + 1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Displays a few of the handwritten digits and their true labels.\n",
    "Confirms we’ve loaded the data correctly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reshape data to fit the CNN input requirements\n",
    "#    CNNs on grayscale images typically expect shape: (height, width, channels).\n",
    "#    For MNIST: height=28, width=28, channels=1 (grayscale).\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# 2. Convert from int to float32 and normalise pixel values from 0-255 to 0-1\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# 3. One-hot encode the labels\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"Data reshaped to:\", X_train.shape, \"and normalised.\")\n",
    "print(\"Labels one-hot encoded to:\", y_train_cat.shape)\n",
    "\n",
    "\"\"\"\n",
    "reshape: (28, 28, 1) indicates 1 colour channel (grayscale).\n",
    "normalise: Dividing by 255 scales values into [0,1], aiding training.\n",
    "one-hot encoding: Each label becomes a 10-element vector for the 10 digit classes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Conv2D: Learns filters (kernels) that detect specific shapes or edges.\n",
    "- MaxPooling2D: Reduces the size of each feature map, capturing the most important information.\n",
    "- Flatten: Turns the 2D (or 3D) features into a single 1D vector to feed into fully connected layers.\n",
    "- Dense: Standard “fully connected” layers for classification.\n",
    "- Dropout: Randomly “turns off” some neurons during training to help prevent overfitting.\n",
    "- softmax: Outputs a probability distribution over the 10 classes.\n",
    "\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer 1:\n",
    "#  - 32 filters, each 3x3 in size\n",
    "#  - 'relu' activation function\n",
    "#  - input shape for the first layer only: (28, 28, 1)\n",
    "model.add(\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1))\n",
    ")\n",
    "\n",
    "# Max Pooling Layer:\n",
    "#  - reduces spatial dimensions, helping to generalise and reduce parameters\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional Layer 2:\n",
    "#  - 64 filters, each 3x3 in size\n",
    "#  - 'relu' activation function\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "\n",
    "# Another Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten: Converts 2D feature maps to a 1D vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense (Fully Connected) layer\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Optional Dropout: helps reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer: 10 neurons (for 10 digit classes),\n",
    "# each with 'softmax' for probabilities\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll train for 10 epochs, which should be enough to get decent accuracy on MNIST.\n",
    "# We can always increase this for better performance.\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat, batch_size=128, epochs=10, validation_split=0.1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\"\"\"\n",
    "- batch_size=128: The network updates weights after seeing 128 images.\n",
    "- epochs=10: Number of times the model sees the entire training dataset.\n",
    "- validation_split=0.1: Splits 10% of training data for validation (monitoring overfitting).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualise how accuracy and loss change as the model trains.\n",
    "Look for signs of overfitting (training accuracy much higher than validation accuracy).\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluates performance on images that were not used during training.\n",
    "MNIST CNNs typically exceed 98% accuracy quite easily.\n",
    "\"\"\"\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities for test images\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "\"\"\"\n",
    "predict returns probabilities for each of the 10 digit classes.\n",
    "We take the argmax (highest probability) to pick the predicted digit.\n",
    "The confusion matrix shows correct predictions on the main diagonal, errors off-diagonal.\n",
    "The classification report shows precision, recall, and F1-score for each digit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise a few random test images alongside predictions\n",
    "indices = np.random.choice(len(X_test), 6, replace=False)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i, idx in enumerate(indices):\n",
    "    img = X_test[idx].reshape(28, 28)  # convert back to 28x28 for display\n",
    "    true_label = y_test[idx]\n",
    "    pred_label = y_pred_classes[idx]\n",
    "\n",
    "    plt.subplot(1, 6, i + 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"True:{true_label}, Pred:{pred_label}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Picks 6 random images from the test set.\n",
    "Shows the true digit vs. the predicted digit.\n",
    "Good for a final sanity check.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Future Experiments\n",
    "\n",
    "**Well done!** You've built a simple Convolutional Neural Network to classify MNIST digits.\n",
    "\n",
    "Next ideas to try:\n",
    "1. Increase or change layers (e.g., more conv layers, different filter sizes).\n",
    "2. Change hyperparameters (learning rate, batch size, epochs).\n",
    "3. Use data augmentation (although MNIST is already easy, it may add variety).\n",
    "4. Try other optimisers (stochastic gradient descent, etc.).\n",
    "5. Explore deeper networks or advanced architectures (e.g., adding more pooling layers).\n",
    "\n",
    "Enjoy experimenting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "image_path = \"test_3.png\"\n",
    "predicted_digit = predict_digit(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
