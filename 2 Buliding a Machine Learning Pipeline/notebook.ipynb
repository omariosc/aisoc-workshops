{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2: Building an ML Pipeline\n",
    "\n",
    "In this notebook, we will learn how we can build machine learning (ML) pipelines for various takes. You will learn:\n",
    "\n",
    "1. Basic Python programming concepts.\n",
    "2. How to import datasets and preprocess different types of data.\n",
    "3. Various data preprocessing techniques:\n",
    "   - Imputation\n",
    "   - Normalisation\n",
    "   - Standardisation\n",
    "   - Noise removal\n",
    "   - Dimensionality Reduction\n",
    "4. Building machine learning models:\n",
    "   - Linear Regression (for regression tasks)\n",
    "   - Logistic Regression (for classification tasks)\n",
    "   - Decision Trees (very powerful and interpretable models)\n",
    "   - Clustering (unsupervised learning)\n",
    "   - A complete machine learning pipeline example\n",
    "5. Basic natural language processing (NLP) tasks:\n",
    "   - Word tokenisation\n",
    "   - Simple embeddings\n",
    "6. How to use prompt engineering and generative AI tools to assist in writing code.\n",
    "\n",
    "Let's begin with some very basic Python concepts. If you are confident, then feel free to skip ahead to the next section.\n",
    "\n",
    "### 1 Basic Python Concepts\n",
    "\n",
    "Make sure you run every cell in this notebook. You can run a cell by clicking on it and pressing `Shift + Enter`, or by clicking the play button on the left side of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In Python, some common keywords are:\n",
    "- if, else, elif\n",
    "- for, while (used for loops)\n",
    "- def (used to define functions)\n",
    "- class (used to define classes)\n",
    "- import (used to import libraries)\n",
    "- return (to return values from a function)\n",
    "\n",
    "Below, we will print a message to show that everything is working well.\n",
    "\"\"\"\n",
    "\n",
    "# Printing \"Hello, World!\" - the classic starter in many programming lessons.\n",
    "# To run this code in Google Colab, click on the play button next to the code.\n",
    "print(\"Hello, World!\")\n",
    "\n",
    "# This is a comment, and it will not be executed.\n",
    "# It is for the programmer to read and understand the code.\n",
    "\n",
    "\"\"\"\n",
    "This is a multi-line comment.\n",
    "It can be used to write multiple lines of comments.\n",
    "It is also not executed by the interpreter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, variables do not need explicit type declarations.\n",
    "\n",
    "# String\n",
    "greeting = \"Hello, students!\"\n",
    "\n",
    "# Integer\n",
    "age = 20\n",
    "\n",
    "# Float\n",
    "temperature = 36.7\n",
    "\n",
    "# Boolean\n",
    "is_python_fun = True\n",
    "\n",
    "# Let's print them out:\n",
    "print(greeting, \"I am\", age, \"years old.\")\n",
    "print(\"The temperature is\", temperature, \"degrees Celsius.\")\n",
    "print(\"Is Python fun?\", is_python_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists are collections of items. They are ordered and changeable.\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "print(\"This is a list:\", my_list)\n",
    "\n",
    "# Dictionaries store key-value pairs, and are unordered.\n",
    "my_dict = {\"name\": \"Alice\", \"age\": 25}\n",
    "print(\"This is a dictionary:\", my_dict)\n",
    "\n",
    "# We can index lists:\n",
    "print(\"The first element in my_list is:\", my_list[0])\n",
    "\n",
    "# We can access dictionary items by key:\n",
    "print(\"Alice's age is:\", my_dict[\"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Datasets and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will import key Python libraries used in data analysis and machine learning:\n",
    "\n",
    "- numpy: for numerical operations\n",
    "- pandas: for data manipulation\n",
    "- matplotlib.pyplot: for plotting graphs\n",
    "- seaborn: for statistical data visualisation\n",
    "- scikit-learn: for machine learning tasks\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plots to show up nicely\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about this piece of code, it is to make sure that the dataset is installed correctly\n",
    "# If you get an error in the following cell, run this cell, restart the kernel and then run the following cell again\n",
    "import certifi\n",
    "import os\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's load a sample dataset using pandas. \n",
    "We're using a built-in dataset from seaborn (tips dataset) for demonstration.\n",
    "\"\"\"\n",
    "\n",
    "# Load the 'tips' dataset from seaborn\n",
    "tips_data = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First five rows:\")\n",
    "print(tips_data.head())\n",
    "\n",
    "# Get basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(tips_data.info())\n",
    "\n",
    "# Describe the numerical columns\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(tips_data.describe())\n",
    "\n",
    "\"\"\"\n",
    "Floating point numbers are basically numbers with decimal points.\n",
    "The category data type is used to represent data that has a fixed number of unique values.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualise selected features of the tips dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Histogram for the total_bill, tip and size\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# kde is set to True to show the kernel density estimate\n",
    "# It appears on our histogram as a smooth line\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(tips_data[\"total_bill\"], kde=True)\n",
    "plt.title(\"Total Bill (£)\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(tips_data[\"tip\"], kde=True)\n",
    "plt.title(\"Tip (£)\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(tips_data[\"size\"], kde=True)\n",
    "plt.title(\"Size (number of people)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The line graph looks like a bell curve, which is a common shape for histograms.\n",
    "\n",
    "Now, what do you observe from the following visualisation?\n",
    "\"\"\"\n",
    "\n",
    "# Pairplot to inspect relationships between 'total_bill', 'tip', and 'size'\n",
    "sns.pairplot(tips_data, vars=['total_bill', 'tip', 'size'], hue='tip', diag_kind='kde')\n",
    "plt.suptitle(\"Pairplot of Total Bill, Tip, and Size\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot a bar chart for sex, smoker day and time.\n",
    "\"\"\"\n",
    "\n",
    "# Create subplots for 'sex', 'smoker', 'day', and 'time'\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Bar chart for 'sex'\n",
    "tips_data[\"sex\"].value_counts().plot(kind=\"bar\", ax=axes[0], color=\"skyblue\")\n",
    "axes[0].set_title(\"Sex\")\n",
    "axes[0].set_xlabel(\"Category\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Bar chart for 'smoker'\n",
    "tips_data[\"smoker\"].value_counts().plot(kind=\"bar\", ax=axes[1], color=\"salmon\")\n",
    "axes[1].set_title(\"Smoker\")\n",
    "axes[1].set_xlabel(\"Category\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Bar chart for 'day'\n",
    "tips_data[\"day\"].value_counts().plot(kind=\"bar\", ax=axes[2], color=\"lightgreen\")\n",
    "axes[2].set_title(\"Day\")\n",
    "axes[2].set_xlabel(\"Category\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "\n",
    "# Bar chart for 'time'\n",
    "tips_data[\"time\"].value_counts().plot(kind=\"bar\", ax=axes[3], color=\"violet\")\n",
    "axes[3].set_title(\"Time\")\n",
    "axes[3].set_xlabel(\"Category\")\n",
    "axes[3].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sometimes, we need to convert categorical data to numerical data.\n",
    "\n",
    "Let's convert the sex, smoker, day and time columns to numerical data.\n",
    "\"\"\"\n",
    "\n",
    "# We can make males 1 and females -1 (Negative doesn't matter, but we want to see if there is a difference between males and females later)\n",
    "tips_data[\"sex\"] = tips_data[\"sex\"].map({\"Male\": 1, \"Female\": -1})\n",
    "\n",
    "# We can make smokers 1 and non-smokers -1\n",
    "tips_data[\"smoker\"] = tips_data[\"smoker\"].map({\"No\": -1, \"Yes\": 1})\n",
    "\n",
    "# We can make days 0, 1, 2, 3\n",
    "tips_data[\"day\"] = tips_data[\"day\"].map({\"Thur\": 0, \"Fri\": 1, \"Sat\": 2, \"Sun\": 3})\n",
    "\n",
    "# We can make lunch -1 and dinner 1\n",
    "tips_data[\"time\"] = tips_data[\"time\"].map({\"Lunch\": -1, \"Dinner\": 1})\n",
    "\n",
    "print(tips_data.head())\n",
    "\n",
    "\"\"\"\n",
    "As an afterthought, if we assume that a higher value means someone is more likely to tip, how could we change these conversions?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's investigate the correlation between numerical features.\n",
    "\n",
    "This will help us understand how features are related to each other.\n",
    "\"\"\"\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = tips_data.corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings (you may see a warning in the next cell\n",
    "# Running this cell should hide it\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see if our dataset has missing values and how to handle them.\n",
    "For demonstration, we'll artificially introduce some missing values.\n",
    "\"\"\"\n",
    "\n",
    "# Introduce missing values in the 'total_bill' column arbitrarily\n",
    "import random\n",
    "\n",
    "random_indices = random.sample(range(len(tips_data)), 5)  # select 5 random rows\n",
    "for idx in random_indices:\n",
    "    tips_data.at[idx, \"total_bill\"] = None  # set total_bill to NaN\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Number of missing values per column:\")\n",
    "print(tips_data.isnull().sum())\n",
    "\n",
    "# Let's replace missing values in 'total_bill' with the column's mean (Imputation)\n",
    "mean_total_bill = tips_data[\"total_bill\"].mean()\n",
    "tips_data[\"total_bill\"].fillna(mean_total_bill, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\nNumber of missing values after imputation:\")\n",
    "print(tips_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'Noise' can mean various things (e.g., outliers, irrelevant data).\n",
    "For a quick demonstration, we can detect outliers visually.\n",
    "We'll create a simple boxplot for 'total_bill'.\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=tips_data[\"total_bill\"])\n",
    "plt.title(\"Boxplot of Total Bill (After Imputation)\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "If we spot unusual outliers, we might remove them, but in this dataset, we can keep it simple.\n",
    "If needed, we could filter out rows with 'total_bill' above a certain threshold.\n",
    "e.g. tips_data = tips_data[tips_data[\"total_bill\"] < 50] (to remove total_bill values above 50)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalisation typically rescales the range of features to [0, 1].\n",
    "Standardisation transforms data so that it has mean 0 and standard deviation 1.\n",
    "We'll demonstrate with the 'total_bill' column. We'll create new columns for demonstration:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Convert total_bill to a 2D numpy array for the scaler (scikit-learn requirement)\n",
    "total_bill_values = tips_data[['total_bill']].values\n",
    "\n",
    "# Normalisation\n",
    "min_max_scaler = MinMaxScaler()\n",
    "tips_data['total_bill_normalised'] = min_max_scaler.fit_transform(total_bill_values)\n",
    "\n",
    "# Standardisation\n",
    "std_scaler = StandardScaler()\n",
    "tips_data['total_bill_standardised'] = std_scaler.fit_transform(total_bill_values)\n",
    "\n",
    "print(tips_data[['total_bill','total_bill_normalised','total_bill_standardised']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will demonstrate Principal Component Analysis (PCA) on a slightly larger numerical dataset.\n",
    "For simplicity, let's just extract numeric columns from 'tips' and do a small PCA.\n",
    "\n",
    "PCA is a technique used to reduce high-dimensional data into fewer features while retaining most of the variation.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extract numeric columns (ignoring nulls for demonstration):\n",
    "numeric_data = tips_data.select_dtypes(include=[np.number]).dropna(axis=1)\n",
    "\n",
    "# We will do a 2-component PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(numeric_data)\n",
    "\n",
    "# pca_result is now reduced to 2 dimensions\n",
    "print(\"Original numeric shape:\", numeric_data.shape)\n",
    "print(\"Reduced shape:\", pca_result.shape)\n",
    "\n",
    "# Show the captured variance in the two components\n",
    "# Ideally, we want to capture most of the variance with fewer components\n",
    "# The explained variance ratio tells us how much variance is captured by each component\n",
    "# The total explained variance is the sum of the explained variance ratios\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_.round(2))\n",
    "print(\"Total explained variance:\", sum(pca.explained_variance_ratio_).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear regression is used to predict a continuous target.\n",
    "We will predict 'total_bill' using 'size' and 'tip'.\n",
    "\n",
    "Steps:\n",
    "1. Prepare features (X) and target (y).\n",
    "2. Split dataset into training and testing sets.\n",
    "3. Train a linear regression model.\n",
    "4. Evaluate performance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Let's keep it very simple: We predict 'total_bill' based on 'tip' and 'size'\n",
    "X = tips_data[[\"tip\", \"size\"]]  # features\n",
    "y = tips_data[\"total_bill\"]  # target\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "# Random state ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create the linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Train (fit) the model on training data\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# Evaluate using Mean Squared Error (MSE)\n",
    "# This is the average of the squared differences between predicted and actual values\n",
    "# Lower values are better, with 0 being perfect\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Display coefficients and intercept\n",
    "print(\"Coefficients:\", lin_reg.coef_)\n",
    "print(\"Intercept:\", lin_reg.intercept_)\n",
    "\n",
    "# Plot the regression line and all points in 3D (to show both features and target)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the actual points\n",
    "ax.scatter(X_test[\"tip\"], X_test[\"size\"], y_test, color='blue', label='Actual')\n",
    "\n",
    "# Plot the predicted points\n",
    "ax.scatter(X_test[\"tip\"], X_test[\"size\"], y_pred, color='red', label='Predicted')\n",
    "\n",
    "ax.set_xlabel('Tip')\n",
    "ax.set_ylabel('Size')\n",
    "ax.set_zlabel('Total Bill')\n",
    "plt.title(\"Regression Plane and Predictions\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "What if we used all the features?\n",
    "\n",
    "Do you think that this model is good, and if not, then why?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Now, add 2 more plots as 3 subplots in one row.\n",
    "We will create:\n",
    "   - Plot 1: 'tip' (predictor) vs. 'total_bill' (target) scatter with best-fit line.\n",
    "   - Plot 2: 'size' (predictor) vs. 'total_bill' scatter with best-fit line.\n",
    "   - Plot 3: Actual total_bill vs. model-predicted total_bill with an identity line.\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ----- Plot 1: Tip vs Total Bill -----\n",
    "x_tip = X_test[\"tip\"]\n",
    "y_actual = y_test\n",
    "\n",
    "axes[0].scatter(x_tip, y_actual, color=\"blue\", label=\"Data Points\")\n",
    "# Compute best-fit line for tip -> total_bill\n",
    "coeffs_tip = np.polyfit(x_tip, y_actual, deg=1)\n",
    "poly_tip = np.poly1d(coeffs_tip)\n",
    "x_line = np.linspace(x_tip.min(), x_tip.max(), 100)\n",
    "axes[0].plot(x_line, poly_tip(x_line), color=\"red\", label=\"Best Fit Line\")\n",
    "axes[0].set_xlabel(\"Tip\")\n",
    "axes[0].set_ylabel(\"Total Bill\")\n",
    "axes[0].set_title(\"Tip vs Total Bill\")\n",
    "axes[0].legend()\n",
    "\n",
    "# ----- Plot 2: Size vs Total Bill -----\n",
    "x_size = X_test[\"size\"]\n",
    "# Using the same y_actual from y_test\n",
    "axes[1].scatter(x_size, y_actual, color=\"green\", label=\"Data Points\")\n",
    "# Compute best-fit line for size -> total_bill\n",
    "coeffs_size = np.polyfit(x_size, y_actual, deg=1)\n",
    "poly_size = np.poly1d(coeffs_size)\n",
    "x_line_size = np.linspace(x_size.min(), x_size.max(), 100)\n",
    "axes[1].plot(x_line_size, poly_size(x_line_size), color=\"orange\", label=\"Best Fit Line\")\n",
    "axes[1].set_xlabel(\"Size\")\n",
    "axes[1].set_ylabel(\"Total Bill\")\n",
    "axes[1].set_title(\"Size vs Total Bill\")\n",
    "axes[1].legend()\n",
    "\n",
    "# ----- Plot 3: Predicted Total Bill vs Actual Total Bill -----\n",
    "# Here, x-axis is actual total_bill and y-axis is predicted. An ideal model will have points lie near the identity line.\n",
    "axes[2].scatter(y_actual, y_pred, color=\"purple\", label=\"Data Points\")\n",
    "# For a reference, plot the identity line (y = x)\n",
    "min_val = min(y_actual.min(), y_pred.min())\n",
    "max_val = max(y_actual.max(), y_pred.max())\n",
    "axes[2].plot(\n",
    "    [min_val, max_val], [min_val, max_val], color=\"gray\", linestyle=\"--\", label=\"Ideal\"\n",
    ")\n",
    "axes[2].set_xlabel(\"Actual Total Bill\")\n",
    "axes[2].set_ylabel(\"Predicted Total Bill\")\n",
    "axes[2].set_title(\"Predicted vs Actual Total Bill\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Discussion:\n",
    "Using only 'tip' and 'size' to predict 'total_bill' gives a simple linear model,\n",
    "but if you used all features (e.g., including categorical variables after proper encoding)\n",
    "the model might change. In our case, the 3D plot and the 2D plots show that there is a clear\n",
    "linear relation with 'tip' and a looser relation with 'size'. However, the prediction accuracy \n",
    "may still be limited because 'total_bill' is affected by many factors (like time, day, and group composition).\n",
    "Therefore, the model might be overly simplistic when using only these two predictors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic regression is used to predict a categorical variable (e.g., 'Yes' or 'No').\n",
    "We will use 'tips' dataset to classify whether the tip was 'high' or 'low'.\n",
    "\n",
    "Steps:\n",
    "1. Create a 'high_tip' label that is 1 if tip > certain threshold, 0 otherwise.\n",
    "2. Split into features (X) and target (y).\n",
    "3. Train a logistic regression model.\n",
    "4. Evaluate performance with accuracy.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Let's create a binary classification: 'high_tip' > $3 => 1, else 0\n",
    "tips_data[\"high_tip\"] = tips_data[\"tip\"].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "# Features: 'total_bill' and 'size'\n",
    "X = tips_data[[\"total_bill\", \"size\"]]\n",
    "y = tips_data[\"high_tip\"]\n",
    "\n",
    "# Split data\n",
    "# X_train: features for training, X_test: features for testing\n",
    "# y_train: target for training, y_test: target for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Classification accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Clustering is an unsupervised learning technique.\n",
    "We'll use K-Means to group similar samples together. \n",
    "Let's cluster the numeric data from tips (just 'total_bill' and 'tip') into 2 clusters.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Extract just two numeric columns\n",
    "clustering_data = tips_data[[\"total_bill\", \"tip\"]].dropna()\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(clustering_data)\n",
    "\n",
    "# Cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add labels back to the dataset\n",
    "clustering_data[\"cluster\"] = labels\n",
    "\n",
    "print(clustering_data.head(10))\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=clustering_data, x=\"total_bill\", y=\"tip\", hue=\"cluster\")\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Does this look right to you?\n",
    "\n",
    "Do we need to have more clusters? What happens if we do add more clusters?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Logistic Regression with Clustered Data.\n",
    "\n",
    "The idea is that now we have a new feature 'cluster' (0 or 1) that we can use to predict 'high_tip'.\n",
    "Since there is less overlap between the clusters, the model might perform better.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Import necessary functions\n",
    "\n",
    "# Use the 'clustering_data' DataFrame that contains the columns 'total_bill', 'tip', and 'cluster'\n",
    "# Here, we use 'total_bill' and 'tip' as predictors, and 'cluster' (0 or 1) as the target.\n",
    "X_cluster = clustering_data[['total_bill', 'tip']]\n",
    "y_cluster = clustering_data['cluster']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train_cluster, X_test_cluster, y_train_cluster, y_test_cluster = train_test_split(\n",
    "    X_cluster, y_cluster, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train logistic regression on the clustered data\n",
    "log_reg_cluster = LogisticRegression()\n",
    "log_reg_cluster.fit(X_train_cluster, y_train_cluster)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_cluster = log_reg_cluster.predict(X_test_cluster)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_cluster = accuracy_score(y_test_cluster, y_pred_cluster)\n",
    "print(\"Logistic Regression Accuracy on Clustered Data:\", accuracy_cluster)\n",
    "\n",
    "# Optionally, display the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_cluster, y_pred_cluster))\n",
    "\n",
    "\"\"\"\n",
    "This cell splits the clustered data into training and testing sets, trains a logistic regression model to predict the cluster (0 or 1) from total_bill and tip, and then evaluates the model by calculating accuracy and displaying the confusion matrix.\n",
    "\n",
    "Extension: What is the receiver operating characteristic (ROC) curve, and how can it be used to evaluate a binary classifier?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's learn about confusion matrics and metrics.\n",
    "\"\"\"\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# (For example, we use the logistic regression model on high_tip classification)\n",
    "# Assuming y_test and y_pred are available from the earlier cell that trained the model.\n",
    "# If you need to compute predictions again, make sure to call the predict() on your model.\n",
    "# For this demonstration, we assume they have been computed.\n",
    "#\n",
    "# Explanation:\n",
    "# A confusion matrix is a table that is used to evaluate the performance of a classification model.\n",
    "# The rows in the matrix represent the true labels (ground truth), while the columns represent the predicted labels.\n",
    "# For a binary classification problem, the confusion matrix has the following form:\n",
    "#\n",
    "#              Predicted Negative    Predicted Positive\n",
    "# True Negative         TN                    FP\n",
    "# True Positive         FN                    TP\n",
    "#\n",
    "# Where:\n",
    "#   - TN (True Negatives): Number of correctly predicted negative class.\n",
    "#   - TP (True Positives): Number of correctly predicted positive class.\n",
    "#   - FP (False Positives): Number of negatives incorrectly predicted as positive.\n",
    "#   - FN (False Negatives): Number of positives incorrectly predicted as negative.\n",
    "#\n",
    "# This breakdown helps you understand not only the overall accuracy of the model,\n",
    "# but also the type of errors it is making. For instance, a high number of false negatives\n",
    "# might indicate that your model is missing important positive cases, which may be more\n",
    "# detrimental in certain applications.\n",
    "#\n",
    "# In addition to the raw counts, we can also generate a classification report which\n",
    "# includes precision, recall, f1-score, and support for each class.\n",
    "\n",
    "# Compute the confusion matrix for the logistic regression classifier:\n",
    "cm = confusion_matrix(y_test_cluster, y_pred_cluster)\n",
    "\n",
    "# Print the confusion matrix as a numpy array (for quick inspection)\n",
    "print(\"Confusion Matrix (raw counts):\")\n",
    "print(cm)\n",
    "\n",
    "# Optionally, you can also print out a classification report:\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cluster, y_pred_cluster))\n",
    "\n",
    "# Visualize the confusion matrix using Seaborn's heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "\n",
    "# Add labels and title to the plot for clarity\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix for High Tip Prediction\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Discussion:\n",
    "The confusion matrix above provides a complete breakdown of our model’s performance.\n",
    "For binary classification:\n",
    "    - Top-left element (cm[0,0]) corresponds to True Negatives (TN).\n",
    "    - Top-right element (cm[0,1]) corresponds to False Positives (FP).\n",
    "    - Bottom-left element (cm[1,0]) corresponds to False Negatives (FN).\n",
    "    - Bottom-right element (cm[1,1]) corresponds to True Positives (TP).\n",
    "\n",
    "By understanding these values, one can determine how frequently the model misclassified instances in each category. \n",
    "This information is crucial for further model improvement, balancing precision and recall, and understanding the cost of misclassifications (e.g., predicting a low tip when it was actually high or vice versa).\n",
    "\n",
    "Can you do the same for the original results, without clustering and see the difference in performance?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Pipeline Example\n",
    "\n",
    "Now, let's **combine** these steps into a **pipeline**. A pipeline is a sequence of steps:\n",
    "1. Data Loading\n",
    "2. Data Cleaning (imputation, noise removal)\n",
    "3. Feature Engineering (normalisation, encoding)\n",
    "4. Model Selection/Training\n",
    "5. Evaluation\n",
    "\n",
    "We'll show an example with **Logistic Regression** as the final model, following a simplified pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\"\"\"\n",
    "Steps in the pipeline:\n",
    "1. Impute missing numeric values with mean.\n",
    "2. Scale numeric features.\n",
    "3. Train logistic regression.\n",
    "\"\"\"\n",
    "\n",
    "# For demonstration, let's create a pipeline for the classification of 'high_tip'.\n",
    "\n",
    "# Our features will again be 'total_bill' and 'size'.\n",
    "numeric_features = ['total_bill', 'size']\n",
    "categorical_features = ['sex', 'smoker', 'day', 'time']\n",
    "\n",
    "# We define a transformer for numeric features:\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Now we combine the transformer into a ColumnTransformer:\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Finally, we build the pipeline with a logistic regression step:\n",
    "clf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X = tips_data[numeric_features + categorical_features]  # from earlier\n",
    "y = tips_data['high_tip']  # from earlier\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "# Fit the pipeline\n",
    "clf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Training accuracy:\", clf_pipeline.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf_pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Prompt Engineering and Generative AI\n",
    "\n",
    "#### Using Generative AI to Fill in Code\n",
    "\n",
    "Sometimes, you might not remember the exact syntax or the best approach. \n",
    "You can use generative AI tools (like ChatGPT or others) to help. \n",
    "Here is an example prompt you could use:\n",
    "\n",
    "**Prompt**:  \n",
    "\"I have a dataset where I want to build a pipeline using `DecisionTreeClassifier`.  \n",
    "1. It should include imputation of missing values.  \n",
    "2. Normalisation of numerical columns.  \n",
    "3. One-hot encoding of categorical columns.  \n",
    "4. Then train a decision tree.  \n",
    "5. Show me how to split data and get accuracy.  \n",
    "Please provide a Python code example.\"\n",
    "\n",
    "By providing such **detailed instructions** (prompt engineering), you can get the AI model to generate a code skeleton which you can tweak and customise.\n",
    "\n",
    "---\n",
    "\n",
    "#### Exercise for Students\n",
    "\n",
    "1. **Build a new pipeline** for classification using a **Random Forest Classifer** instead of **Logistic Regression**. You may want to look up what is a random forest.  \n",
    "2. Include the following steps in your pipeline:\n",
    "   - Impute missing numeric values.\n",
    "   - Scale numeric features.\n",
    "   - Encode categorical variables (e.g., 'sex' from tips dataset).\n",
    "   - Train a Decision Tree Classifier.\n",
    "\n",
    "**Remember**: Good prompt engineering involves specifying each step you need so that the AI can return exactly what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write or paste your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Natural Language Processing (Optional Extension Task)\n",
    "\n",
    "Try going through the following examples of NLP. \n",
    "\n",
    "You can also try using generative AI to help you perform different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Natural Language Processing often starts with tokenisation (splitting text into words).\n",
    "We can embed words as numerical vectors for machine learning.\n",
    "\n",
    "For demonstration, let's split a simple sentence into words and show a basic embedding idea.\n",
    "\"\"\"\n",
    "\n",
    "sentence = \"Hello there! This is an example sentence for tokenisation.\"\n",
    "\n",
    "# Simple tokenisation by splitting on whitespace\n",
    "tokens = sentence.split()\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# A very naive embedding approach: map each word to an index (like a dictionary)\n",
    "word_to_index = {}\n",
    "current_index = 0\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in word_to_index:\n",
    "        word_to_index[word] = current_index\n",
    "        current_index += 1\n",
    "\n",
    "print(\"Word to index mapping:\", word_to_index)\n",
    "\n",
    "# In practice, you'd use libraries like NLTK, spaCy, or advanced models (like Word2Vec, BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "──────────────────────────────\n",
    "Exercise 1: Train a Word2Vec Model on a Sample Corpus\n",
    "──────────────────────────────\n",
    "• Prepare a small text corpus (or use an existing one such as from NLTK’s “reuters” or “brown” corpora)  \n",
    "• Tokenize the text into sentences and then words  \n",
    "• Train a gensim Word2Vec model on the tokenized data  \n",
    "• List the vocabulary and check similar words for a given word\n",
    "\"\"\"\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Download NLTK tokenizer data if needed\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Define a sample corpus\n",
    "corpus = [\n",
    "    \"Artificial intelligence and machine learning are transforming the world.\",\n",
    "    \"Deep learning is a subset of machine learning that uses neural networks.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Word embeddings are an approach to model the semantics of words.\",\n",
    "    \"Generative models create new data instances similar to the training data.\",\n",
    "]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,  # tokenized sentences\n",
    "    vector_size=50, # size of the word vectors\n",
    "    window=3, # context window size\n",
    "    min_count=1, # minimum count of words\n",
    "    seed=42, # random seed for reproducibility\n",
    ")\n",
    "print(\"Vocabulary of the model:\", list(model.wv.index_to_key))\n",
    "\n",
    "# For example, find words similar to 'machine'\n",
    "print(\"Words similar to 'machine':\", model.wv.most_similar(\"machine\", topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "──────────────────────────────\n",
    "Exercise 2: Visualize the Word Embeddings Using t-SNE\n",
    "──────────────────────────────\n",
    "• Extract the word vectors and reduce their dimensionality using t-SNE  \n",
    "• Create a scatter plot with annotations\n",
    "\"\"\"\n",
    "\n",
    "# Get the list of words and corresponding vectors\n",
    "words = list(model.wv.index_to_key)\n",
    "word_vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "# Reduce dimensions to 2D using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=3)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the 2D word embeddings\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], color=\"purple\")\n",
    "\n",
    "# Add annotations for each point\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]),\n",
    "        xytext=(5, 2),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.title(\"Word2Vec Embeddings Visualised with t-SNE\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "──────────────────────────────\n",
    "Exercise 3: Experiment with Different Parameters\n",
    "──────────────────────────────\n",
    "• Use a larger text corpus (for example, using the NLTK “brown” corpus)  \n",
    "• Adjust parameters like vector_size, window, and min_count  \n",
    "• Plot and compare the resulting embeddings\n",
    "\n",
    "Example code snippet using Brown corpus:\n",
    "\"\"\"\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "# Use a subset (or all) of the sentences from the Brown corpus\n",
    "sentences = brown.sents(categories=\"news\")\n",
    "# For speed, we take a subset (e.g., first 500 sentences)\n",
    "subset_sentences = [\" \".join(sent) for sent in sentences[:500]]\n",
    "tokenized_subset = [nltk.word_tokenize(sentence.lower()) for sentence in subset_sentences]\n",
    "\n",
    "# Train a Word2Vec model on the Brown subset\n",
    "brown_model = Word2Vec(\n",
    "    sentences=tokenized_subset,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=2,\n",
    "    seed=42,\n",
    ")\n",
    "print(\"Vocabulary size from Brown corpus:\", len(brown_model.wv.index_to_key))\n",
    "\n",
    "# Visualize the embeddings (selecting a sample of words for clarity)\n",
    "sample_words = brown_model.wv.index_to_key[:50]  # taking first 50 words\n",
    "sample_vectors = np.array([brown_model.wv[word] for word in sample_words])\n",
    "\n",
    "tsne_brown = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "sample_vectors_2d = tsne_brown.fit_transform(sample_vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(sample_vectors_2d[:, 0], sample_vectors_2d[:, 1], color=\"darkgreen\")\n",
    "for i, word in enumerate(sample_words):\n",
    "    plt.annotate(\n",
    "        word,\n",
    "        xy=(sample_vectors_2d[i, 0], sample_vectors_2d[i, 1]),\n",
    "        xytext=(5, 2),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "plt.title(\"Brown Corpus Word Embeddings Visualised with t-SNE\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "──────────────────────────────\n",
    "Exercise 4: Explore Similarity and Analogy\n",
    "──────────────────────────────\n",
    "• Once you have trained the model, experiment with functions like:\n",
    "    • model.wv.similarity(word1, word2) – to compute similarity between two words  \n",
    "    • model.wv.most_similar(positive=[...], negative=[...]) – to find compelling analogies\n",
    "• Write a cell that computes and prints the similarity between some word pairs and analogies\n",
    "\"\"\"\n",
    "\n",
    "# Similarity between two words\n",
    "similarity = model.wv.similarity('machine', 'learning')\n",
    "print(\"Similarity between 'machine' and 'learning':\", similarity)\n",
    "\n",
    "# Analogy: 'king' - 'man' + 'woman' (if such words exist in your corpus)\n",
    "# (If not, pick words from your vocabulary that make sense)\n",
    "try:\n",
    "    analogy = model.wv.most_similar(positive=['learning'], negative=['machine'], topn=3)\n",
    "    print(\"Analogy result:\", analogy)\n",
    "except KeyError as e:\n",
    "    print(\"One of the words was not found in the vocabulary:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
